# ‚öôÔ∏è Scraper Configuration Profiles

Dieser Guide zeigt dir drei vorkonfigurierte Profile f√ºr unterschiedliche Anwendungsf√§lle. W√§hle das Profil, das zu deinem Usecase passt.

---

## üìã √úbersicht der Profile

| Profil | QPI | Date Restrict | Runtime | Leads | Kosten | Empfohlen f√ºr |
|--------|-----|---------------|---------|-------|--------|---------------|
| **Safe Mode** | 6 | d30 | 3-5 min | 10-30 | ‚Ç¨0 | Erste Tests, keine API-Keys |
| **Balanced Mode** | 12 | d60 | 8-12 min | 30-80 | ‚Ç¨0-0.20 | Standard-Betrieb, t√§gliche Runs |
| **Aggressive Mode** | 20 | d90 | 15-25 min | 80-200 | ‚Ç¨0.50-1.00 | Voller Durchsatz, mit Proxy empfohlen |

---

## üõ°Ô∏è Safe Mode (Starter & Test)

**Ideal f√ºr:**
- üéØ Erste Tests ohne Risiko
- üÜì Keine API-Keys erforderlich
- üß™ Verst√§ndnis f√ºr den Workflow
- üìä Kleine Lead-Mengen f√ºr Qualit√§tschecks

### Konfiguration

#### .env Datei

**Option 1: Vorkonfigurierte Vorlage nutzen (Empfohlen)**
```bash
# Kopiere die Safe Mode Vorlage
cp .env.example.safe .env

# Bearbeite nur SECRET_KEY (erforderlich)
nano .env
```

**Option 2: Manuell konfigurieren**
```bash
# Minimal Configuration (Safe Mode)
SECRET_KEY=your-secret-key-here
DEBUG=False
ALLOWED_HOSTS=localhost,127.0.0.1

# Scraper Settings
SCRAPER_MODE=once
SCRAPER_QPI=6
SCRAPER_DATE_RESTRICT=d30
SCRAPER_DEFAULT_INDUSTRY=recruiter

# Optional (nicht erforderlich f√ºr Safe Mode)
OPENAI_API_KEY=
PERPLEXITY_API_KEY=
```

#### Command-Line
```bash
# Docker
docker-compose --profile scraper up scraper

# Manuell
python scriptname.py --once --industry recruiter --qpi 6 --daterestrict d30
```

#### Im CRM
1. Gehe zu: http://localhost:8000/crm/scraper/
2. Einstellungen:
   - **Modus:** Once
   - **Industry:** recruiter
   - **QPI:** 6
   - **Date Restrict:** d30
3. Klicke "Start Scraper"

### Erwartete Ergebnisse

```
‚è±Ô∏è  Laufzeit:        3-5 Minuten
üìä Leads:            10-30
üí∞ Kosten:           ‚Ç¨0
üîç Queries:          6
üåê API Calls:        0 (ohne OpenAI)
üìà Lead Quality:     Mittel (ohne AI-Enrichment)
‚ö†Ô∏è  Rate Limits:     Sehr niedrig
üîí Block-Risiko:     Minimal
```

### Best Practices

‚úÖ **Gut f√ºr:**
- Ersten Durchlauf zum Testen
- Verst√§ndnis der Datenstruktur
- √úberpr√ºfung der CRM-Integration
- Setup-Validierung

‚ùå **Nicht gut f√ºr:**
- Produktiveinsatz mit hohem Volumen
- Schnelle Lead-Generierung
- Vollst√§ndige Marktabdeckung

---

## ‚öñÔ∏è Balanced Mode (Standard-Betrieb)

**Ideal f√ºr:**
- üè¢ T√§glicher Produktivbetrieb
- üìà Gute Balance zwischen Qualit√§t und Quantit√§t
- üí∞ Geringe bis moderate Kosten
- üîÑ Regelm√§√üige, planbare Runs

### Konfiguration

#### .env Datei

**Option 1: Vorkonfigurierte Vorlage nutzen (Empfohlen)**
```bash
# Kopiere die Balanced Mode Vorlage
cp .env.example.balanced .env

# Bearbeite die Werte:
# - SECRET_KEY (erforderlich)
# - OPENAI_API_KEY (empfohlen)
# - ALLOWED_HOSTS (f√ºr Produktion)
nano .env
```

**Option 2: Manuell konfigurieren**
```bash
# Balanced Configuration
SECRET_KEY=your-secret-key-here
DEBUG=False
ALLOWED_HOSTS=localhost,127.0.0.1,yourdomain.com

# Scraper Settings
SCRAPER_MODE=once
SCRAPER_QPI=12
SCRAPER_DATE_RESTRICT=d60
SCRAPER_DEFAULT_INDUSTRY=recruiter

# Optional: AI f√ºr bessere Lead-Qualit√§t
OPENAI_API_KEY=sk-your-key-here
PERPLEXITY_API_KEY=pplx-your-key-here
```

#### Command-Line
```bash
# Docker
docker-compose --profile scraper up scraper

# Manuell
python scriptname.py --once --industry recruiter --qpi 12 --daterestrict d60
```

#### Im CRM
1. Gehe zu: http://localhost:8000/crm/scraper/
2. Einstellungen:
   - **Modus:** Once
   - **Industry:** recruiter
   - **QPI:** 12
   - **Date Restrict:** d60
3. Klicke "Start Scraper"

### Erwartete Ergebnisse

```
‚è±Ô∏è  Laufzeit:        8-12 Minuten
üìä Leads:            30-80
üí∞ Kosten:           ‚Ç¨0.10-0.30 (mit OpenAI)
üîç Queries:          12
üåê API Calls:        ~30-80 (f√ºr AI-Enrichment)
üìà Lead Quality:     Hoch (mit AI)
‚ö†Ô∏è  Rate Limits:     Moderat
üîí Block-Risiko:     Gering
```

### Best Practices

‚úÖ **Empfohlene Frequenz:**
- 1x t√§glich (z.B. morgens 6 Uhr)
- 2-3x pro Woche f√ºr geringeren Durchsatz
- Automatisiert via Cron oder Scheduler

‚úÖ **Optimierungen:**
```bash
# Mehrere Industries parallel
python scriptname.py --once --industry recruiter --qpi 12 &
python scriptname.py --once --industry talent_hunt --qpi 8 &
wait

# Mit zus√§tzlichen Filtern
python scriptname.py --once --industry recruiter --qpi 12 --daterestrict d60 --min-score 70
```

‚úÖ **Monitoring:**
- √úberwache Lead-Quality √ºber CRM-Dashboard
- Pr√ºfe API-Kosten w√∂chentlich
- Adjustiere QPI basierend auf Results

---

## üöÄ Aggressive Mode (Maximaler Durchsatz)

**Ideal f√ºr:**
- üí™ Maximale Lead-Generierung
- üéØ Schnelle Marktabdeckung
- üîÑ One-Time Campaigns
- üåê Mit Proxy-Infrastruktur

### ‚ö†Ô∏è Wichtige Voraussetzungen

Bevor du Aggressive Mode nutzt:

1. **Proxy oder VPN empfohlen:**
   - Reduziert Block-Risiko
   - Verteilt Requests auf mehrere IPs
   - Siehe: [Proxy Setup Guide](../PROXY_FIX_SUMMARY.md)

2. **API-Keys konfiguriert:**
   - OpenAI f√ºr AI-Enrichment
   - Google Custom Search API (optional, f√ºr mehr Queries)

3. **Monitoring aktiv:**
   - Live-Logs √ºberwachen
   - Error-Rate beobachten
   - Bei >20% 403s: Pause einlegen

### Konfiguration

#### .env Datei

**Option 1: Vorkonfigurierte Vorlage nutzen (Empfohlen)**
```bash
# Kopiere die Aggressive Mode Vorlage
cp .env.example.aggressive .env

# Bearbeite die Werte (alle erforderlich!):
# - SECRET_KEY
# - OPENAI_API_KEY (erforderlich f√ºr Aggressive)
# - HTTP_PROXY / HTTPS_PROXY (dringend empfohlen)
# - ALLOWED_HOSTS (f√ºr Produktion)
nano .env
```

**Option 2: Manuell konfigurieren**
```bash
# Aggressive Configuration
SECRET_KEY=your-secret-key-here
DEBUG=False
ALLOWED_HOSTS=localhost,127.0.0.1,yourdomain.com

# Scraper Settings
SCRAPER_MODE=once
SCRAPER_QPI=20
SCRAPER_DATE_RESTRICT=d90
SCRAPER_DEFAULT_INDUSTRY=recruiter

# Required: AI APIs
OPENAI_API_KEY=sk-your-key-here
PERPLEXITY_API_KEY=pplx-your-key-here

# Optional: Google CSE f√ºr mehr Queries
GOOGLE_API_KEY=your-google-key
GOOGLE_CSE_ID=your-cse-id

# Optional: Proxy Configuration (siehe Proxy Guide)
HTTP_PROXY=http://your-proxy:port
HTTPS_PROXY=https://your-proxy:port
```

#### Command-Line
```bash
# Mit allen Features
python scriptname.py --once --industry recruiter --qpi 20 --daterestrict d90

# Mehrere Industries gleichzeitig
python scriptname.py --once --industry recruiter --qpi 20 --daterestrict d90 &
python scriptname.py --once --industry talent_hunt --qpi 15 --daterestrict d90 &
python scriptname.py --once --industry callcenter --qpi 10 --daterestrict d90 &
wait
```

#### Im CRM
1. Gehe zu: http://localhost:8000/crm/scraper/
2. Einstellungen:
   - **Modus:** Once
   - **Industry:** recruiter
   - **QPI:** 20
   - **Date Restrict:** d90
3. Klicke "Start Scraper"
4. **Wichtig:** √úberwache die Live-Logs!

### Erwartete Ergebnisse

```
‚è±Ô∏è  Laufzeit:        15-25 Minuten
üìä Leads:            80-200
üí∞ Kosten:           ‚Ç¨0.50-1.50 (mit OpenAI)
üîç Queries:          20+
üåê API Calls:        ~100-200 (f√ºr AI-Enrichment)
üìà Lead Quality:     Sehr hoch (mit AI)
‚ö†Ô∏è  Rate Limits:     Hoch
üîí Block-Risiko:     Mittel-Hoch (ohne Proxy)
```

### Best Practices

‚úÖ **Vorbereitung:**
- Proxy/VPN aktivieren
- API-Keys validieren
- Monitoring Dashboard √∂ffnen
- Zeitfenster planen (nicht zu Spitzenzeiten)

‚úÖ **W√§hrend des Runs:**
- Live-Logs beobachten
- Bei >20% Fehlerrate: Pause (Ctrl+C)
- Bei Blocks: Proxy wechseln oder Delay erh√∂hen

‚úÖ **Nach dem Run:**
- Lead-Quality pr√ºfen (Score-Verteilung)
- Duplikate entfernen (automatisch)
- API-Kosten tracken
- Lessons learned dokumentieren

‚ùå **Nicht empfohlen:**
- Ohne Proxy/VPN auf Heimnetzwerk
- Mehrfach t√§glich (Block-Risiko!)
- Ohne Monitoring
- Ohne API-Budget

---

## üîß Custom Configuration

Du kannst auch eigene Profile erstellen:

### Eigenes Profil definieren

```bash
# Beispiel: "Weekend Warrior" - Samstags gro√üer Run
SCRAPER_QPI=15
SCRAPER_DATE_RESTRICT=d45
SCRAPER_DEFAULT_INDUSTRY=recruiter

# Beispiel: "Nightly Crawl" - Nachts moderate Runs
SCRAPER_QPI=10
SCRAPER_DATE_RESTRICT=d30
SCRAPER_DEFAULT_INDUSTRY=talent_hunt
```

### Parameter-Referenz

| Parameter | Werte | Beschreibung |
|-----------|-------|--------------|
| `SCRAPER_MODE` | `once`, `continuous` | Einmaliger Run vs. Dauerbetrieb |
| `SCRAPER_QPI` | 1-30 | Queries per Industry (h√∂her = mehr Leads) |
| `SCRAPER_DATE_RESTRICT` | `d7`, `d30`, `d60`, `d90` | Zeitfenster f√ºr Suchergebnisse |
| `SCRAPER_DEFAULT_INDUSTRY` | `recruiter`, `talent_hunt`, `callcenter` | Ziel-Industry |

### Industries verf√ºgbar

```bash
# B2B Vertriebskontakte
--industry recruiter

# Aktive Jobsuchende im Sales
--industry talent_hunt

# Callcenter & Telemarketing
--industry callcenter

# Construction Industry
--industry construction

# Medical Industry
--industry medical

# Food & Beverage
--industry food
```

---

## üìä Performance-Vergleich

### Lead-Quality nach Profil

```
Safe Mode:       ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (Mittel, ohne AI)
Balanced Mode:   ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Hoch, mit AI)
Aggressive Mode: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Sehr hoch, mit AI + Volume)
```

### Kosten-Nutzen-Analyse

```
Safe Mode:       Cost/Lead: ‚Ç¨0      (kein AI, keine API)
Balanced Mode:   Cost/Lead: ‚Ç¨0.005  (50 Leads f√ºr ‚Ç¨0.25)
Aggressive Mode: Cost/Lead: ‚Ç¨0.008  (150 Leads f√ºr ‚Ç¨1.20)
```

### Block-Risiko

```
Safe Mode:       üü¢ Sehr gering (6 Queries)
Balanced Mode:   üü° Gering (12 Queries, moderate Frequenz)
Aggressive Mode: üî¥ Mittel-Hoch (20+ Queries, Proxy empfohlen)
```

---

## üÜò Troubleshooting

### "Zu wenige Leads" (< 10 bei Balanced)

```bash
# Check 1: Date Restrict erweitern
SCRAPER_DATE_RESTRICT=d90

# Check 2: QPI erh√∂hen
SCRAPER_QPI=15

# Check 3: Andere Industry testen
--industry talent_hunt
```

### "Zu viele 403 Errors"

```bash
# L√∂sung 1: QPI reduzieren
SCRAPER_QPI=8

# L√∂sung 2: Delay erh√∂hen (im Script)
# Edit scriptname.py: DELAY_BETWEEN_REQUESTS = 3

# L√∂sung 3: Proxy aktivieren
HTTP_PROXY=http://your-proxy:port
```

### "API-Kosten zu hoch"

```bash
# L√∂sung 1: AI-Enrichment deaktivieren
OPENAI_API_KEY=  # leer lassen

# L√∂sung 2: QPI reduzieren
SCRAPER_QPI=6

# L√∂sung 3: Weniger h√§ufige Runs
# z.B. nur 2x pro Woche statt t√§glich
```

**Mehr Hilfe:** [Troubleshooting Guide](TROUBLESHOOTING.md)

---

## üìö Weiterf√ºhrende Dokumentation

- **[Quickstart Guide](QUICKSTART.md)** - Erste Schritte in 20 Minuten
- **[Installation Guide](INSTALLATION.md)** - Detaillierte Setup-Anleitung
- **[Deployment Guide](DEPLOYMENT.md)** - Produktiv-Deployment
- **[Troubleshooting](TROUBLESHOOTING.md)** - Probleml√∂sungen

---

**Feedback?** [GitHub Issues](https://github.com/sundsoffice-tech/luca-nrw-scraper/issues) | **Fragen?** [README.md](../README.md)
