name: Nightly Scraper Run

on:
  schedule:
    # T√§glich um 02:00 UTC (entspricht 03:00 CET im Winter, 04:00 CEST im Sommer)
    # Hinweis: F√ºr 03:00 CET ganzj√§hrig (inkl. Sommerzeit) w√§re '0 1 * * *' n√∂tig
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      qpi:
        description: 'Queries pro Branche (Queries Per Industry)'
        required: false
        default: '6'
        type: string
      industry:
        description: 'Branche (all, recruiter, candidates, talent_hunt)'
        required: false
        default: 'all'
        type: string
      dry_run:
        description: 'Nur Scrapen, nicht importieren'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  scrape:
    name: Run Scraper
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      actions: write  # For artifact upload
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          
          # Django dependencies f√ºr Import
          pip install -r telis_recruitment/requirements.txt
      
      - name: Create output directory
        run: mkdir -p output
      
      - name: Run Scraper
        id: scraper
        env:
          QPI: ${{ github.event.inputs.qpi || '6' }}
          INDUSTRY: ${{ github.event.inputs.industry || 'all' }}
        run: |
          # Run scraper once
          python scriptname.py --once --qpi $QPI --industry $INDUSTRY || true
          
          # Check if CSV was created
          if [ -f "vertrieb_kontakte.csv" ]; then
            LEAD_COUNT=$(tail -n +2 vertrieb_kontakte.csv | wc -l)
            echo "leads_found=$LEAD_COUNT" >> $GITHUB_OUTPUT
            echo "csv_exists=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Scraper completed: $LEAD_COUNT leads found"
          else
            echo "leads_found=0" >> $GITHUB_OUTPUT
            echo "csv_exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No CSV file created"
          fi
      
      - name: Import to Django
        if: steps.scraper.outputs.csv_exists == 'true' && github.event.inputs.dry_run != 'true'
        env:
          DJANGO_SETTINGS_MODULE: telis.settings
          SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY || 'github-actions-temp-key' }}
          DEBUG: 'False'
          DATABASE_URL: ${{ secrets.DATABASE_URL || 'sqlite:///db.sqlite3' }}
        run: |
          cd telis_recruitment
          
          # Run migrations if needed
          python manage.py migrate --no-input
          
          # Import CSV
          python manage.py import_scraper_csv ../vertrieb_kontakte.csv
          
          echo "‚úÖ Import completed"
      
      - name: Upload CSV Artifact
        if: steps.scraper.outputs.csv_exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: leads-${{ github.run_number }}-${{ github.run_attempt }}
          path: vertrieb_kontakte.csv
          retention-days: 30
      
      - name: Upload XLSX Artifact (if exists)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: leads-xlsx-${{ github.run_number }}-${{ github.run_attempt }}
          path: vertrieb_kontakte.xlsx
          if-no-files-found: ignore
          retention-days: 30
      
      - name: Create Summary
        if: always()
        run: |
          echo "## ü§ñ Scraper Run Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Run Date | $(date -u '+%Y-%m-%d %H:%M UTC') |" >> $GITHUB_STEP_SUMMARY
          echo "| Leads Found | ${{ steps.scraper.outputs.leads_found || '0' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| CSV Created | ${{ steps.scraper.outputs.csv_exists || 'false' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Dry Run | ${{ github.event.inputs.dry_run || 'false' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.scraper.outputs.csv_exists }}" == "true" ]; then
            echo "‚úÖ **Scraper completed successfully!**" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è **No leads found in this run**" >> $GITHUB_STEP_SUMMARY
          fi


  notify:
    name: Send Notification
    runs-on: ubuntu-latest
    needs: scrape
    if: always()
    permissions: {}
    
    steps:
      - name: Notify on Slack (optional)
        if: env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          STATUS="${{ needs.scrape.result }}"
          if [ "$STATUS" == "success" ]; then
            EMOJI="‚úÖ"
            COLOR="good"
          else
            EMOJI="‚ùå"
            COLOR="danger"
          fi
          
          curl -X POST $SLACK_WEBHOOK_URL \
            -H 'Content-type: application/json' \
            -d "{
              \"attachments\": [{
                \"color\": \"$COLOR\",
                \"title\": \"$EMOJI Nightly Scraper Run\",
                \"text\": \"Status: $STATUS\",
                \"footer\": \"GitHub Actions\",
                \"ts\": $(date +%s)
              }]
            }" || true
