"""
Crawler-Funktionen für verschiedene Portale
"""
import asyncio
import re
from typing import List, Dict, Callable, Optional, Any
from bs4 import BeautifulSoup

# ============================================
# KLEINANZEIGEN CRAWLER
# ============================================

async def crawl_kleinanzeigen_listings_async(
    listing_url: str,
    max_pages: int = 5,
    http_get_func: Callable = None,
    log_func: Callable = None,
    normalize_func: Callable = None,
    ENABLE_KLEINANZEIGEN:  bool = True,
    HTTP_TIMEOUT: int = 25,
    PORTAL_DELAYS: Dict = None,
    jitter_func: Callable = None,
) -> List[str]:
    """
    Crawlt Kleinanzeigen Listing-Seiten und extrahiert Detail-URLs.
    """
    if not ENABLE_KLEINANZEIGEN:
        return []

    if log_func:
        log_func("debug", f"Crawling Kleinanzeigen: {listing_url}")

    detail_urls = []

    for page in range(1, max_pages + 1):
        if page == 1:
            url = listing_url
        else: 
            if "seite:" in listing_url:
                url = re.sub(r'seite:\d+', f'seite:{page}', listing_url)
            else:
                url = f"{listing_url}/seite:{page}"

        try:
            if jitter_func and PORTAL_DELAYS:
                delay = PORTAL_DELAYS.get("kleinanzeigen", 2. 0)
                await asyncio. sleep(jitter_func(delay))

            if http_get_func:
                html = await http_get_func(url, timeout=HTTP_TIMEOUT)
                if not html:
                    break

                soup = BeautifulSoup(html, 'html. parser')
                found_on_page = 0
                for link in soup.select('a[href*="/s-anzeige/"]'):
                    href = link.get('href', '')
                    if href and '/s-anzeige/' in href:
                        full_url = f"https://www.kleinanzeigen.de{href}" if href.startswith('/') else href
                        if normalize_func:
                            full_url = normalize_func(full_url)
                        if full_url not in detail_urls:
                            detail_urls. append(full_url)
                            found_on_page += 1

                if log_func:
                    log_func("debug", f"Kleinanzeigen Seite {page}:  {found_on_page} URLs gefunden")

                if found_on_page == 0:
                    break

        except Exception as e:
            if log_func:
                log_func("warn", f"Kleinanzeigen crawl error: {e}")
            break

    if log_func:
        log_func("info", f"Kleinanzeigen:  {len(detail_urls)} Detail-URLs gesammelt", url=listing_url)

    return detail_urls


async def extract_kleinanzeigen_detail_async(
    url: str,
    http_get_func: Callable = None,
    log_func: Callable = None,
    normalize_phone_func: Callable = None,
    validate_phone_func: Callable = None,
    is_mobile_number_func: Callable = None,
    extract_all_phone_patterns_func: Callable = None,
    get_best_phone_number_func: Callable = None,
    extract_whatsapp_number_func: Callable = None,
    extract_phone_with_browser_func:  Callable = None,
    extract_name_enhanced_func: Callable = None,
    learning_engine = None,
    HTTP_TIMEOUT: int = 25,
    EMAIL_RE = None,
    MOBILE_RE = None,
) -> Optional[Dict[str, Any]]:
    """
    Extrahiert Lead-Daten von einer Kleinanzeigen Detail-Seite.
    VOLLSTÄNDIGE IMPLEMENTIERUNG mit Telefon-Extraktion! 
    """
    try:
        if not http_get_func:
            return None

        html = await http_get_func(url, timeout=HTTP_TIMEOUT)
        if not html: 
            return None

        soup = BeautifulSoup(html, 'html.parser')
        text = soup.get_text(" ", strip=True)

        data = {
            'quelle': url,
            'source_url': url,
            'source':  'kleinanzeigen',
            'lead_type': 'candidate',
        }

        title_el = soup.select_one('h1#viewad-title')
        if title_el: 
            data['title'] = title_el.get_text(strip=True)

        desc_el = soup.select_one('#viewad-description-text')
        description = ""
        if desc_el: 
            description = desc_el.get_text(strip=True)
            data['description'] = description

        name = ""
        name_el = soup. select_one('. userprofile-vip a')
        if name_el: 
            name = name_el.get_text(strip=True)
        if not name:
            name_el = soup.select_one('. iconlist--text')
            if name_el:
                name = name_el.get_text(strip=True)
        if extract_name_enhanced_func and (not name or len(name) < 3):
            try:
                enhanced_name = extract_name_enhanced_func(text, url)
                if enhanced_name and len(enhanced_name) > len(name or ""):
                    name = enhanced_name
            except Exception: 
                pass
        if name: 
            data['name'] = name

        loc_el = soup. select_one('#viewad-locality')
        if loc_el:
            location = loc_el.get_text(strip=True)
            data['region'] = location
            data['location'] = location

        telefon = ""

        if extract_whatsapp_number_func: 
            try:
                wa_number = extract_whatsapp_number_func(html)
                if wa_number: 
                    telefon = wa_number
                    data['whatsapp_link'] = f"https://wa.me/{wa_number. replace('+', '')}"
                    if log_func:
                        log_func("debug", "Kleinanzeigen:  WhatsApp gefunden", url=url)
            except Exception: 
                pass

        if not telefon:
            wa = soup.select_one('a[href*="wa.me"], a[href*="api.whatsapp.com"]')
            if wa:
                href = wa.get("href", "")
                tel_raw = re.sub(r'\D', '', href)
                if tel_raw and len(tel_raw) >= 10:
                    telefon = "+" + tel_raw
                    data['whatsapp_link'] = href

        if not telefon and extract_all_phone_patterns_func: 
            try:
                phones = extract_all_phone_patterns_func(text)
                if phones and get_best_phone_number_func:
                    best = get_best_phone_number_func(phones)
                    if best:
                        telefon = best
            except Exception:
                pass

        if not telefon:
            phone_patterns = [
                r'(\+49\s*1[567]\d[\s\-/]*\d{7,})',
                r'(01[567]\d[\s\-/]*\d{6,})',
                r'(\+49\s*\(0\)\s*1[567]\d[\s\-/]*\d{6,})',
            ]
            for pattern in phone_patterns:
                match = re.search(pattern, text)
                if match:
                    raw = match.group(1)
                    cleaned = re.sub(r'[\s\-/]', '', raw)
                    if len(cleaned) >= 10:
                        telefon = cleaned
                        break

        if not telefon and extract_phone_with_browser_func:
            try:
                browser_phone = await extract_phone_with_browser_func(url)
                if browser_phone:
                    telefon = browser_phone
                    if log_func:
                        log_func("debug", "Kleinanzeigen: Telefon via Browser extrahiert", url=url)
            except Exception:
                pass

        if telefon and normalize_phone_func:
            try:
                telefon = normalize_phone_func(telefon)
            except Exception:
                pass

        if telefon and validate_phone_func: 
            try:
                is_valid, phone_type = validate_phone_func(telefon)
                if is_valid: 
                    data['telefon'] = telefon
                    data['phone_type'] = phone_type
                    if is_mobile_number_func: 
                        data['is_mobile'] = is_mobile_number_func(telefon)
            except Exception:
                data['telefon'] = telefon
        elif telefon:
            data['telefon'] = telefon

        if EMAIL_RE:
            email_match = EMAIL_RE.search(text)
            if email_match:
                data['email'] = email_match.group(0)

        if data. get('telefon'):
            if log_func:
                log_func("info", "Kleinanzeigen: Lead extrahiert",
                        url=url,
                        has_phone=True,
                        name=data.get('name', '')[:20])
            return data
        else:
            if log_func:
                log_func("debug", "Kleinanzeigen:  Kein Telefon gefunden", url=url)
            return None

    except Exception as e:
        if log_func:
            log_func("warn", f"Kleinanzeigen detail error: {e}", url=url)
        return None


async def crawl_kleinanzeigen_portal_async(
    urls: List[str],
    http_get_func: Callable = None,
    log_func:  Callable = None,
    **kwargs
) -> List[Dict]:
    results = []
    for url in urls:
        detail_urls = await crawl_kleinanzeigen_listings_async(
            listing_url=url,
            http_get_func=http_get_func,
            log_func=log_func,
            **kwargs
        )
        for detail_url in detail_urls:
            data = await extract_kleinanzeigen_detail_async(
                url=detail_url,
                http_get_func=http_get_func,
                log_func=log_func,
                **kwargs
            )
            if data:
                results.append(data)
    return results


def _mark_url_seen(
    url: str,
    source:  str = "",
    db_func: Callable = None,
    log_func: Callable = None,
    seen_cache:  set = None,
    normalize_func: Callable = None,
) -> bool:
    try:
        normalized = normalize_func(url) if normalize_func else url
        if seen_cache is not None:
            seen_cache. add(normalized)
        if db_func:
            con = db_func()
            cur = con.cursor()
            cur.execute('''
                INSERT OR IGNORE INTO seen_urls (url, source, seen_at)
                VALUES (?, ?, datetime('now'))
            ''', (normalized, source))
            con.commit()
            con.close()
        return True
    except Exception as e:
        if log_func:
            log_func("warn", f"Failed to mark URL seen: {e}")
        return False


async def crawl_markt_de_listings_async(*args, **kwargs) -> List[str]:
    return []

async def crawl_quoka_listings_async(*args, **kwargs) -> List[str]:
    return []

async def crawl_kalaydo_listings_async(*args, **kwargs) -> List[str]:
    return []

async def crawl_meinestadt_listings_async(*args, **kwargs) -> List[str]:
    return []

async def extract_generic_detail_async(*args, **kwargs) -> Optional[Dict]:
    return None


__all__ = [
    'crawl_kleinanzeigen_listings_async',
    'extract_kleinanzeigen_detail_async',
    'crawl_kleinanzeigen_portal_async',
    'crawl_markt_de_listings_async',
    'crawl_quoka_listings_async',
    'crawl_kalaydo_listings_async',
    'crawl_meinestadt_listings_async',
    'extract_generic_detail_async',
    '_mark_url_seen',
]
