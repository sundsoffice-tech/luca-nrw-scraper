import subprocess
import time
import json
import sqlite3
from datetime import datetime
from pathlib import Path

print("=" * 70)
print("DIAGNOSE-RUN GESTARTET")
print(f"Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Dauer: 30 Minuten")
print("=" * 70)

start_time = time.time()
duration_minutes = 30
duration_seconds = duration_minutes * 60

metrics = {
    "start": datetime.now().isoformat(),
    "runs": 0,
    "total_leads_saved": 0,
    "errors": [],
    "warnings": [],
    "skipped_portals": {},
    "skipped_queries":  0,
    "http_errors": {"403": 0, "429": 0, "5xx": 0, "timeout":  0},
    "portal_results": {},
    "rejected_leads": [],
}

log_file = Path("diagnose_run.log")
log_file.write_text("")

def parse_log_line(line):
    global metrics
    
    if "[LEARNING] Skipping" in line:
        portal = line.split("Skipping")[1].split("(")[0].strip()
        metrics["skipped_portals"][portal] = metrics["skipped_portals"].get(portal, 0) + 1
    
    if "Query bereits erledigt (skip)" in line:
        metrics["skipped_queries"] += 1
    
    if "403" in line: 
        metrics["http_errors"]["403"] += 1
    if "429" in line:
        metrics["http_errors"]["429"] += 1
    if "5xx" in line:
        metrics["http_errors"]["5xx"] += 1
    if "timeout" in line. lower():
        metrics["http_errors"]["timeout"] += 1
    
    if "Leads extrahiert" in line or "crawl complete" in line:
        for p in ["Kleinanzeigen", "Quoka", "Markt", "DHD24", "Freelancermap", "Freelance"]:
            if p.lower() in line.lower():
                try:
                    if '"count": ' in line:
                        count = int(line.split('"count":')[1].split("}")[0].split(",")[0].strip())
                        metrics["portal_results"][p] = metrics["portal_results"].get(p, 0) + count
                except:
                    pass
    
    if "Lead abgelehnt" in line: 
        metrics["rejected_leads"]. append(line.strip()[:100])
    
    if "[ERROR" in line or "[FATAL" in line:
        metrics["errors"]. append(line.strip()[:150])
    
    if "[WARN" in line:
        metrics["warnings"].append(line.strip()[:150])
    
    if "Run #" in line and "gestartet" in line:
        metrics["runs"] += 1
    
    if "neue Leads gespeichert" in line: 
        try:
            count = int(line.split('"count":')[1].split("}")[0].split(",")[0].strip())
            metrics["total_leads_saved"] += count
        except:
            pass

print("\nStarte Scraper im Dauerbetrieb.. .\n")

process = subprocess.Popen(
    ["python", "scriptname.py", "--daemon", "--industry", "candidates", "--qpi", "25", "--smart"],
    stdout=subprocess.PIPE,
    stderr=subprocess.STDOUT,
    text=True,
    bufsize=1
)

try:
    while time.time() - start_time < duration_seconds:
        line = process.stdout.readline()
        if line:
            print(line, end="")
            parse_log_line(line)
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(line)
        
        elapsed = int(time.time() - start_time)
        if elapsed > 0 and elapsed % 300 == 0:
            remaining = duration_seconds - elapsed
            print(f"\n--- {elapsed//60} Min vergangen, noch {remaining//60} Min uebrig ---\n")

except KeyboardInterrupt:
    print("\nManuell abgebrochen!")

finally:
    process.terminate()
    try:
        process.wait(timeout=5)
    except:
        process.kill()

metrics["end"] = datetime.now().isoformat()
metrics["duration_minutes"] = round((time.time() - start_time) / 60, 1)

try:
    conn = sqlite3.connect("scraper.db")
    metrics["db_total_leads"] = conn.execute("SELECT COUNT(*) FROM leads").fetchone()[0]
    metrics["db_today_leads"] = conn.execute(
        "SELECT COUNT(*) FROM leads WHERE date(scraped_at) = date('now')"
    ).fetchone()[0]
    conn.close()
except Exception as e:
    metrics["db_error"] = str(e)

print("\n")
print("=" * 70)
print("DIAGNOSE-REPORT")
print("=" * 70)

print(f"""
LAUFZEIT
  Dauer: {metrics['duration_minutes']} Minuten
  Runs: {metrics['runs']}

LEADS
  Gespeichert: {metrics['total_leads_saved']}
  Abgelehnt: {len(metrics['rejected_leads'])}
  In DB gesamt: {metrics. get('db_total_leads', 'N/A')}
  Heute neu: {metrics.get('db_today_leads', 'N/A')}

PORTAL-ERGEBNISSE""")

for portal, count in sorted(metrics['portal_results'].items(), key=lambda x: -x[1]):
    status = "[OK]" if count > 0 else "[FAIL]"
    print(f"  {status} {portal}: {count} Leads")

print("\nGESKIPPTE PORTALE")
if metrics['skipped_portals']: 
    for portal, count in metrics['skipped_portals'].items():
        print(f"  SKIP {portal}: {count}x")
else:
    print("  Keine Portale geskippt")

print(f"""
QUERIES
  Geskippt (Cache): {metrics['skipped_queries']}

HTTP-FEHLER
  403 (Forbidden): {metrics['http_errors']['403']}
  429 (Rate Limit): {metrics['http_errors']['429']}
  5xx (Server): {metrics['http_errors']['5xx']}
  Timeout:  {metrics['http_errors']['timeout']}

ERRORS:  {len(metrics['errors'])}
WARNINGS: {len(metrics['warnings'])}
""")

print("=" * 70)
print("HANDLUNGSEMPFEHLUNGEN")
print("=" * 70)

if metrics['skipped_portals']: 
    print("  1. Portal-Skipping Logik fixen")
if metrics['skipped_queries'] > 20:
    print("  2. Query-Cache zuruecksetzen")
if metrics['http_errors']['403'] > 5:
    print("  3. VPN fuer geblockte Seiten")
if metrics['http_errors']['429'] > 5:
    print("  4. Rate-Limiting erhoehen")
if len(metrics['rejected_leads']) > metrics['total_leads_saved']:
    print("  5. Lead-Validierung lockern")
if metrics['total_leads_saved'] == 0:
    print("  KRITISCH: Keine Leads gespeichert!")

report_file = Path(f"diagnose_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
with open(report_file, "w", encoding="utf-8") as f:
    json.dump(metrics, f, indent=2, ensure_ascii=False)

print(f"\nReport gespeichert: {report_file}")
print(f"Log-Datei: {log_file}")
print("=" * 70)